\section{Thiết lập môi trường và cấu hình}

\subsection{Cài đặt môi trường Python}
Dự án sử dụng Python 3.10 để đảm bảo tương thích tốt nhất với MediaPipe và PyTorch.

\begin{aivncodebox}
\begin{python}
# Tạo môi trường ảo Python 3.10
conda create -n gesture_env python=3.10
# Kích hoạt môi trường
conda activate gesture_env
# Kiểm tra version Python
python --version
\end{python}
\end{aivncodebox}

\subsection{Cài đặt thư viện cần thiết}
Cài đặt các thư viện để chạy hệ thống nhận diện cử chỉ tay và điều khiển relay.

\begin{aivncodebox}
\begin{python}
cd gesture_recognization/
pip install -r requirements.txt
# Kiểm tra cài đặt thành công
python -c "import torch, cv2, mediapipe, yaml; print('Setup OK!')"
\end{python}
\end{aivncodebox}

\subsection{File cấu hình config.yaml}
File \texttt{config.yaml} chứa tất cả cấu hình của hệ thống:

\begin{aivncodebox}
\begin{python}
# Định nghĩa các cử chỉ tay (6 loại)
gestures:
  0: "turn_off"      # Tắt tất cả relay
  1: "light1_on"     # Bật relay 1
  2: "light1_off"    # Tắt relay 1  
  3: "light2_on"     # Bật relay 2
  4: "light2_off"    # Tắt relay 2
  5: "turn_on"       # Bật tất cả relay

# Cài đặt nhận diện tay
sensor_settings:
  max_hands: 1                  # Chỉ nhận diện 1 tay
  detection_confidence: 0.7     # Độ tin cậy phát hiện tay
  tracking_confidence: 0.5      # Độ tin cậy theo dõi tay

# Cài đặt mô hình neural network
model_settings:
  input_size: 63               # 21 landmarks x 3 tọa độ
  hidden_sizes: [128, 64, 32]  # Kích thước các lớp ẩn
  dropout: 0.2                 # Tỷ lệ dropout
  learning_rate: 0.001         # Tốc độ học

# Cài đặt relay
relay_settings:
  baudrate: 9600              # Tốc độ truyền dữ liệu
  timeout: 2                  # Thời gian chờ
  default_port: "COM3"        # Cổng serial mặc định
\end{python}
\end{aivncodebox}

\subsection{Cấu trúc dự án}
Dự án được tái cấu trúc theo chức năng để dễ bảo trì và mở rộng:

\begin{verbatim}
AIO2025/
    gesture_recognization/
        common/
            models.py               # Các lớp và hàm dùng chung
        hardware/
            modbus_controller.py    # Lớp ModbusMaster cho relay
        processing/
            data_collector.py       # Thu thập dữ liệu landmarks
        training/
            gesture_trainer_recognizer.py  # Huấn luyện model
            hand_gesture_recgonition.ipynb # Notebook nghiên cứu
            models/                 # Model đã huấn luyện (.pth)
        app/
            main_controller.py      # Ứng dụng chính
        data/
            landmarks_train.csv     # Dữ liệu huấn luyện
            landmarks_test.csv      # Dữ liệu kiểm tra
            landmarks_val.csv       # Dữ liệu validation
        config.yaml                 # Cấu hình cử chỉ
        requirements.txt            # Thư viện Python
        README.md                   # Hướng dẫn sử dụng
    main.tex                        # Tài liệu dự án (LaTeX)
    README.md                       # Hướng dẫn tổng thể
\end{verbatim}


% =====================
% MODULE CHUNG - COMMON
% =====================

\section{Module Chung (Common)}

Module này chứa các class được sử dụng chung cho cả 3 bước. Mục đích là tránh viết lại code và dễ bảo trì.

\subsection{Class HandLandmarksDetector}
\textbf{Mục đích:} Phát hiện và trích xuất 21 điểm đặc trưng trên bàn tay

\begin{aivncodebox}
\begin{python}
import cv2
import mediapipe as mp
import numpy as np
from typing import List, Tuple, Optional

class HandLandmarksDetector:
    """Phát hiện và trích xuất tọa độ 21 điểm trên bàn tay"""
    
    def __init__(self, detection_confidence=0.7, tracking_confidence=0.5):
        # Khởi tạo MediaPipe
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,        # Xử lý video (không phải ảnh)
            max_num_hands=1,                # Chỉ nhận diện 1 tay
            min_detection_confidence=detection_confidence,
            min_tracking_confidence=tracking_confidence
        )
        self.mp_draw = mp.solutions.drawing_utils
    
    def detect_hand(self, frame: np.ndarray) -> Tuple[Optional[List], np.ndarray]:
        """
        Phát hiện tay và trả về tọa độ 21 điểm landmarks
        
        Args:
            frame: Khung hình từ camera (BGR format)
            
        Returns:
            landmarks_list: Danh sách tọa độ 21 điểm (x,y,z) hoặc None
            annotated_frame: Khung hình có vẽ landmarks
        """
        # Chuyển đổi BGR sang RGB (MediaPipe yêu cầu)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Phát hiện tay
        results = self.hands.process(rgb_frame)
        
        # Sao chép frame để vẽ landmarks
        annotated_frame = frame.copy()
        landmarks_list = []
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # Vẽ landmarks lên frame
                self.mp_draw.draw_landmarks(
                    annotated_frame, 
                    hand_landmarks, 
                    self.mp_hands.HAND_CONNECTIONS
                )
                
                # Trích xuất tọa độ
                landmarks = []
                for landmark in hand_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
                landmarks_list.append(landmarks)
        
        return landmarks_list if landmarks_list else None, annotated_frame
\end{python}
\end{aivncodebox}

\subsection{Class HandGestureModel}
\textbf{Mục đích:} Mô hình neural network để phân loại cử chỉ tay

\begin{aivncodebox}
\begin{python}
import torch
import torch.nn as nn

class HandGestureModel(nn.Module):
    """Mô hình neural network cho phân loại cử chỉ tay"""
    
    def __init__(self, input_size=63, num_classes=6):
        super(HandGestureModel, self).__init__()
        
        # Mạng neural network với 4 lớp
        self.network = nn.Sequential(
            nn.Linear(input_size, 256),    # Lớp 1: 63 -> 256
            nn.ReLU(),                     # Hàm kích hoạt
            nn.Dropout(0.1),               # Dropout tránh overfitting
            
            nn.Linear(256, 128),           # Lớp 2: 256 -> 128
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(128, 64),            # Lớp 3: 128 -> 64
            nn.ReLU(),
            
            nn.Linear(64, num_classes)     # Lớp output: 64 -> 6
        )
    
    def forward(self, x):
        """Tính toán forward pass"""
        return self.network(x)
    
    def predict(self, x, threshold=0.7):
        """
        Dự đoán với ngưỡng tin cậy
        
        Args:
            x: Input tensor
            threshold: Ngưỡng tin cậy (0.7 = 70%)
            
        Returns:
            predicted: Class được dự đoán hoặc -1 nếu không tin cậy
        """
        with torch.no_grad():
            outputs = self.forward(x)
            probabilities = torch.softmax(outputs, dim=1)
            max_prob, predicted = torch.max(probabilities, 1)
            
            # Trả về -1 nếu tin cậy thấp
            if max_prob.item() < threshold:
                return torch.tensor(-1)
            return predicted
\end{python}
\end{aivncodebox}

\subsection{Function load config}
\textbf{Mục đích:} Đọc cấu hình từ file config.yaml

\begin{aivncodebox}
\begin{python}
import yaml
from typing import Dict

def label_dict_from_config_file(config_path: str = "config.yaml") -> Dict[int, str]:
    """
    Đọc danh sách gesture từ file config
    
    Args:
        config_path: Đường dẫn file config.yaml
        
    Returns:
        Dict mapping từ ID gesture sang tên gesture
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as file:
            config = yaml.safe_load(file)
            return config.get('gestures', {})
    except FileNotFoundError:
        print(f"Không tìm thấy file: {config_path}")
        return {}
    except Exception as e:
        print(f"Lỗi đọc config: {e}")
        return {}
\end{python}
\end{aivncodebox}



% =====================
% MODULE THU THẬP DỮ LIỆU (PROCESSING)
% =====================

\section{Module Thu Thập Dữ Liệu (Processing)}

Module này chứa các công cụ để thu thập dữ liệu cử chỉ tay từ camera và lưu trữ dưới dạng landmarks CSV.

\subsection{Class HandDatasetWriter}
\textbf{Mục đích:} Ghi dữ liệu landmarks vào file CSV

\begin{aivncodebox}
\begin{python}
import os
import csv
from typing import List

class HandDatasetWriter:
    """Ghi dữ liệu landmarks vào file CSV"""
    
    def __init__(self, file_path: str):
        # Tạo thư mục nếu chưa có
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        self.file_path = file_path
        self.sample_count = 0
        
        # Mở file CSV để ghi
        self.csv_file = open(self.file_path, 'w', newline="", encoding="utf-8")
        self.file_writer = csv.writer(self.csv_file, delimiter=',')
        
        # Tạo header: x0,y0,z0,x1,y1,z1,...,x20,y20,z20,label
        header = []
        for i in range(21):  # 21 landmarks
            header.extend([f'x{i}', f'y{i}', f'z{i}'])
        header.append('label')  # Cột cuối là nhãn
        
        self.file_writer.writerow(header)
        print(f'Tạo file CSV: {self.file_path}')
    
    def add(self, hand_landmarks: List[float], label: int):
        """
        Thêm một mẫu dữ liệu vào CSV
        
        Args:
            hand_landmarks: List 63 số (21 landmarks x 3 tọa độ)
            label: Nhãn cử chỉ (0-5)
        """
        if len(hand_landmarks) != 63:
            raise ValueError(f"Cần 63 landmarks, nhận được {len(hand_landmarks)}")
        
        # Tạo row: [x0,y0,z0,...,x20,y20,z20,label]
        row = hand_landmarks + [label]
        self.file_writer.writerow(row)
        self.sample_count += 1
        
        # Hiển thị tiến trình mỗi 50 mẫu
        if self.sample_count % 50 == 0:
            print(f"Đã thu thập {self.sample_count} mẫu")
    
    def close(self):
        """Đóng file và hiển thị thống kê"""
        self.csv_file.close()
        print(f"Hoàn thành! Tổng cộng: {self.sample_count} mẫu")
        print(f"File lưu tại: {self.file_path}")
\end{python}
\end{aivncodebox}

\subsection{Class GestureDataCollector}
\textbf{Mục đích:} Điều khiển quá trình thu thập dữ liệu từ camera

\begin{aivncodebox}
\begin{python}
import cv2
from gesture_recognization.common.models import HandLandmarksDetector, label_dict_from_config_file

class GestureDataCollector:
    """Điều khiển thu thập dữ liệu cử chỉ tay"""
    
    def __init__(self, config_path: str = "../config.yaml"):
        # Đọc danh sách gesture từ config
        self.gesture_labels = label_dict_from_config_file(config_path)
        
        # Khởi tạo detector
        self.detector = HandLandmarksDetector()
        
        print("=== THU THẬP DỮ LIỆU CỬ CHỈ TAY ===")
        print(f"Có {len(self.gesture_labels)} loại cử chỉ:")
        for gid, gname in self.gesture_labels.items():
            print(f"  Phím '{chr(ord('a') + gid)}': {gname}")
    
    def collect_data(self, mode: str):
        """
        Thu thập dữ liệu cho một dataset (train/val/test)
        
        Args:
            mode: Loại dataset ('train', 'val', 'test')
        """
        print(f"\n=== THU THẬP DỮ LIỆU {mode.upper()} ===")
        print("Cách sử dụng:")
        print("- Nhấn 'a' đến 'f' để chọn cử chỉ")
        print("- Nhấn lần 2 để bắt đầu/dừng ghi")
        print("- Nhấn 'q' để thoát")
        print("=" * 40)
        
        # Tạo file CSV
        data_folder = "./data"
        os.makedirs(data_folder, exist_ok=True)
        csv_file = os.path.join(data_folder, f"landmarks_{mode}.csv")
        writer = HandDatasetWriter(csv_file)
        
        # Khởi tạo camera
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        # Trạng thái thu thập
        current_label = None    # Cử chỉ hiện tại
        is_recording = False    # Có đang ghi không?
        frame_count = 0         # Số frame đã ghi
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Không đọc được từ camera!")
                break
            
            # Phát hiện tay
            landmarks, annotated_frame = self.detector.detect_hand(frame)
            
            # Ghi dữ liệu nếu đang recording
            if is_recording and landmarks and current_label is not None:
                writer.add(landmarks[0], current_label)
                frame_count += 1
                
                # Hiển thị trạng thái ghi
                cv2.putText(annotated_frame, f"DANG GHI: {frame_count}", 
                           (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.circle(annotated_frame, (50, 120), 20, (0, 0, 255), -1)
            
            # Hiển thị trạng thái hiện tại
            if current_label is not None:
                gesture_name = self.gesture_labels.get(current_label, "Unknown")
                status = f"Cu chi: {gesture_name} | Ghi: {'BAT' if is_recording else 'TAT'}"
                cv2.putText(annotated_frame, status, (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # Hiển thị có phát hiện tay không
            if landmarks:
                cv2.putText(annotated_frame, "PHAT HIEN TAY", 
                           (10, annotated_frame.shape[0] - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            cv2.imshow(f'Thu thap du lieu - {mode.upper()}', annotated_frame)
            
            # Xử lý phím bấm
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif ord('a') <= key <= ord('f'):
                # Chọn cử chỉ (a=0, b=1, ..., f=5)
                new_label = key - ord('a')
                if new_label in self.gesture_labels:
                    if current_label == new_label:
                        # Bật/tắt ghi cho cử chỉ hiện tại
                        is_recording = not is_recording
                        gesture_name = self.gesture_labels[new_label]
                        if is_recording:
                            frame_count = 0
                            print(f"Bắt đầu ghi: {gesture_name}")
                        else:
                            print(f"Dừng ghi. Đã ghi {frame_count} mẫu")
                    else:
                        # Chọn cử chỉ mới
                        current_label = new_label
                        is_recording = False
                        frame_count = 0
                        gesture_name = self.gesture_labels[new_label]
                        print(f"Chọn cử chỉ: {gesture_name}")
        
        # Dọn dẹp
        cap.release()
        cv2.destroyAllWindows()
        writer.close()
        print(f"Hoàn thành thu thập dữ liệu {mode}")
\end{python}
\end{aivncodebox}

\subsection{Cách chạy thu thập dữ liệu}

\begin{aivncodebox}
\begin{python}
# Chạy thu thập dữ liệu từ thư mục gốc
cd gesture_recognization/

# Thu thập training data
python -c "
from processing.data_collector import GestureDataCollector
collector = GestureDataCollector()
collector.collect_data('train')  # Thu thập nhiều nhất (70%)
"

# Thu thập validation data  
python -c "
from processing.data_collector import GestureDataCollector
collector = GestureDataCollector()
collector.collect_data('val')    # Thu thập 15%
"

# Thu thập test data
python -c "
from data_collector import GestureDataCollector
collector = GestureDataCollector()
collector.collect_data('test')   # Thu thập 15%
"
\end{python}
\end{aivncodebox}

\subsection{Hướng dẫn thu thập}

\textbf{Bước 1: Chuẩn bị}
\begin{itemize}
    \item Ngồi cách camera 50-80cm
    \item Ánh sáng đều, nền đơn giản
    \item Tay sạch, không đeo trang sức
\end{itemize}

\textbf{Bước 2: Thu thập từng cử chỉ}
\begin{itemize}
    \item Nhấn phím 'a' đến 'f' để chọn cử chỉ
    \item Nhấn phím lần 2 để bắt đầu ghi
    \item Thực hiện cử chỉ liên tục 200-500 lần
    \item Nhấn phím lần 3 để dừng ghi
\end{itemize}

\textbf{Bước 3: Kiểm tra kết quả}
\begin{itemize}
    \item File CSV sẽ có 64 cột (63 landmarks + 1 label)
    \item Mỗi cử chỉ nên có ít nhất 200 mẫu
    \item Train data nên có nhiều mẫu nhất
\end{itemize}

% =====================
% MODULE HUẤN LUYỆN (TRAINING)
% =====================

\section{Module Huấn Luyện (Training)}

Module này chứa các công cụ để huấn luyện mô hình neural network từ dữ liệu landmarks đã thu thập.

\subsection{Class HandGestureDataset}
\textbf{Mục đích:} Chuyển đổi dữ liệu CSV thành PyTorch Dataset

\begin{aivncodebox}
\begin{python}
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset

class HandGestureDataset(Dataset):
    """Dataset PyTorch cho dữ liệu cử chỉ tay"""
    
    def __init__(self, csv_path: str):
        print(f"Đang tải dữ liệu từ: {csv_path}")
        
        # Đọc file CSV
        self.data = pd.read_csv(csv_path)
        print(f"Kích thước dữ liệu: {self.data.shape}")
        
        # Tách features (63 cột đầu) và labels (cột cuối)
        self.features = self.data.iloc[:, :-1].values.astype(np.float32)
        self.labels = self.data.iloc[:, -1].values.astype(np.int64)
        
        # Hiển thị thống kê
        unique_labels = np.unique(self.labels)
        print(f"Các lớp có trong dữ liệu: {unique_labels}")
        for label in unique_labels:
            count = np.sum(self.labels == label)
            print(f"  Lớp {label}: {count} mẫu")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # Trả về (features, label) dưới dạng tensor
        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])
\end{python}
\end{aivncodebox}

\subsection{Class HandGestureTrainer}
\textbf{Mục đích:} Điều khiển quá trình huấn luyện mô hình

\begin{aivncodebox}
\begin{python}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from datetime import datetime
import os
from gesture_recognization.common.models import HandGestureModel, label_dict_from_config_file

class HandGestureTrainer:
    """Điều khiển huấn luyện mô hình cử chỉ tay"""
    
    def __init__(self, config_path: str = "../config.yaml"):
        # Đọc cấu hình
        self.gesture_labels = label_dict_from_config_file(config_path)
        self.num_classes = len(self.gesture_labels)
        
        print("=== HUẤN LUYỆN MÔ HÌNH ===")
        print(f"Số lớp cử chỉ: {self.num_classes}")
        for gid, gname in self.gesture_labels.items():
            print(f"  {gid}: {gname}")
    
    def prepare_data(self, batch_size: int = 32):
        """Chuẩn bị dữ liệu train và validation"""
        train_path = "data/landmarks_train.csv"
        val_path = "data/landmarks_val.csv"
        
        # Kiểm tra file tồn tại
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"Không tìm thấy: {train_path}")
        if not os.path.exists(val_path):
            raise FileNotFoundError(f"Không tìm thấy: {val_path}")
        
        # Tạo dataset
        train_dataset = HandGestureDataset(train_path)
        val_dataset = HandGestureDataset(val_path)
        
        # Tạo data loader
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader
    
    def train_model(self, train_loader, val_loader, epochs=200):
        """Huấn luyện mô hình"""
        print(f"\nBắt đầu huấn luyện {epochs} epochs...")
        
        # Khởi tạo mô hình
        model = HandGestureModel(input_size=63, num_classes=self.num_classes)
        criterion = nn.CrossEntropyLoss()  # Hàm loss
        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer
        
        # Lưu mô hình tốt nhất
        best_val_loss = float('inf')
        best_model_state = None
        
        for epoch in range(epochs):
            # === TRAINING PHASE ===
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                
                # Forward pass
                output = model(data)
                loss = criterion(output, target)
                
                # Backward pass
                loss.backward()
                optimizer.step()
                
                # Thống kê
                train_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                train_total += target.size(0)
                train_correct += (predicted == target).sum().item()
            
            # === VALIDATION PHASE ===
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    output = model(data)
                    loss = criterion(output, target)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(output.data, 1)
                    val_total += target.size(0)
                    val_correct += (predicted == target).sum().item()
            
            # Tính toán metrics
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_accuracy = 100 * train_correct / train_total
            val_accuracy = 100 * val_correct / val_total
            
            # Hiển thị kết quả mỗi 10 epoch
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}:")
                print(f"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%")
                print(f"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%")
            
            # Lưu mô hình tốt nhất
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = model.state_dict().copy()
                if (epoch + 1) % 10 == 0:
                    print(f"  -> Mô hình tốt nhất mới!")
        
        # Load mô hình tốt nhất
        if best_model_state:
            model.load_state_dict(best_model_state)
            print(f"\nĐã load mô hình tốt nhất (Val Loss: {best_val_loss:.4f})")
        
        return model
    
    def save_model(self, model, save_path="./models"):
        """Lưu mô hình đã huấn luyện"""
        os.makedirs(save_path, exist_ok=True)
        
        # Tạo tên file với timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f"hand_gesture_model_{timestamp}.pth"
        model_path = os.path.join(save_path, model_filename)
        
        # Lưu mô hình và metadata
        torch.save({
            'model_state_dict': model.state_dict(),
            'gesture_labels': self.gesture_labels,
            'num_classes': self.num_classes,
            'input_size': 63,
            'timestamp': timestamp
        }, model_path)
        
        # Lưu thêm bản copy với tên cố định
        default_path = os.path.join(save_path, "hand_gesture_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'gesture_labels': self.gesture_labels,
            'num_classes': self.num_classes,
            'input_size': 63,
            'timestamp': timestamp
        }, default_path)
        
        print(f"Mô hình đã lưu:")
        print(f"  Có timestamp: {model_path}")
        print(f"  Mặc định: {default_path}")
        
        return model_path
\end{python}
\end{aivncodebox}

\subsection{Cách chạy huấn luyện}

\begin{aivncodebox}
\begin{python}
# Chạy huấn luyện mô hình
cd gesture_recognization/

python -c "
from training.gesture_trainer_recognizer import HandGestureTrainer

# Khởi tạo trainer
trainer = HandGestureTrainer()

# Chuẩn bị dữ liệu
print('Chuẩn bị dữ liệu...')
train_loader, val_loader = trainer.prepare_data()

# Huấn luyện mô hình
print('Bắt đầu huấn luyện...')
model = trainer.train_model(train_loader, val_loader)

# Lưu mô hình
model_path = trainer.save_model(model)
print(f'Hoàn thành! Mô hình đã lưu tại: {model_path}')
"
\end{python}
\end{aivncodebox}

\subsection{Kiểm tra mô hình}

\begin{aivncodebox}
\begin{python}
# Kiểm tra mô hình đã huấn luyện
import torch
import pandas as pd
from sklearn.metrics import classification_report

# Load mô hình
model_path = "./models/hand_gesture_model.pth"
checkpoint = torch.load(model_path, map_location='cpu')
print(f"Mô hình được huấn luyện lúc: {checkpoint['timestamp']}")
print(f"Số lớp: {checkpoint['num_classes']}")

# Kiểm tra trên test set
test_data = pd.read_csv("data/landmarks_test.csv")
features = test_data.iloc[:, :-1].values
labels = test_data.iloc[:, -1].values

# Tính accuracy
from gesture_recognization.common.models import HandGestureModel
model = HandGestureModel(num_classes=checkpoint['num_classes'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

with torch.no_grad():
    predictions = model(torch.tensor(features, dtype=torch.float32))
    _, predicted = torch.max(predictions, 1)
    accuracy = (predicted == torch.tensor(labels)).float().mean()
    print(f"Độ chính xác trên test set: {accuracy:.4f}")
\end{python}
\end{aivncodebox}

\subsection{Giải thích quá trình huấn luyện}

\textbf{Kiến trúc mô hình:}
\begin{itemize}
    \item Input: 63 features (21 landmarks × 3 tọa độ)
    \item Hidden layers: 256 → 128 → 64 neurons
    \item Output: 6 classes (số lượng cử chỉ)
    \item Activation: ReLU, Dropout: 0.1
\end{itemize}

\textbf{Quá trình huấn luyện:}
\begin{itemize}
    \item Optimizer: Adam với learning rate 0.001
    \item Loss function: CrossEntropyLoss
    \item Batch size: 32
    \item Epochs: 200 (có thể dừng sớm)
\end{itemize}

\textbf{Đánh giá mô hình:}
\begin{itemize}
    \item Theo dõi loss và accuracy trên train/val sets
    \item Lưu mô hình có validation loss thấp nhất
    \item Kiểm tra cuối cùng trên test set
\end{itemize}

% =====================
% MODULE ỨNG DỤNG CHÍNH (APP)
% =====================

\section{Module Ứng Dụng Chính (App)}

Module này chứa ứng dụng chính để nhận diện cử chỉ tay real-time và điều khiển relay thông qua Modbus RTU.

\subsection{Class ModbusMaster}
\textbf{Mục đích:} Giao tiếp với relay module qua giao thức Modbus RTU (nằm trong hardware/modbus\_controller.py)

\begin{aivncodebox}
\begin{python}
import serial

class ModbusMaster:
    """Điều khiển relay qua Modbus RTU RS485"""
    
    def __init__(self, port):
        # Kết nối serial với relay module
        self.ser = serial.Serial(
            port=port,
            baudrate=9600,    # Tốc độ chuẩn cho relay
            timeout=2
        )
        print(f"Kết nối relay tại {port} - 9600 baud")
    
    def switch_relay_1(self, state):
        """Điều khiển relay 1"""
        if state:
            # Lệnh bật relay 1: [1,5,0,0,0xFF,0,0x8C,0x3A]
            cmd = [1, 5, 0, 0, 0xFF, 0, 0x8C, 0x3A]
        else:
            # Lệnh tắt relay 1: [1,5,0,0,0,0,0xCD,0xCA]
            cmd = [1, 5, 0, 0, 0, 0, 0xCD, 0xCA]
        
        self.ser.write(bytearray(cmd))
        print(f"Relay 1: {'BẬT' if state else 'TẮT'}")
    
    def switch_relay_2(self, state):
        """Điều khiển relay 2"""
        if state:
            cmd = [1, 5, 0, 1, 0xFF, 0, 0xDD, 0xFA]
        else:
            cmd = [1, 5, 0, 1, 0, 0, 0x9C, 0x0A]
        
        self.ser.write(bytearray(cmd))
        print(f"Relay 2: {'BẬT' if state else 'TẮT'}")
    
    def all_on(self):
        """Bật tất cả relay"""
        self.switch_relay_1(True)
        self.switch_relay_2(True)
        print("Tất cả relay: BẬT")
    
    def all_off(self):
        """Tắt tất cả relay"""
        self.switch_relay_1(False)
        self.switch_relay_2(False)
        print("Tất cả relay: TẮT")
    
    def close(self):
        """Đóng kết nối"""
        self.ser.close()
\end{python}
\end{aivncodebox}

\subsection{Class RelayGestureController}
\textbf{Mục đích:} Điều khiển relay dựa trên cử chỉ tay được nhận diện

\begin{aivncodebox}
\begin{python}
import cv2
import time
import torch
import numpy as np
from gesture_recognization.common.models import (
    HandGestureModel, 
    HandLandmarksDetector, 
    label_dict_from_config_file
)

class RelayGestureController:
    """Điều khiển relay bằng cử chỉ tay real-time"""
    
    def __init__(self, model_path, port=None, simulation=False):
        print("=== ĐIỀU KHIỂN RELAY BẰNG CỬ CHỈ TAY ===")
        
        # Load cấu hình gesture
        self.gesture_labels = label_dict_from_config_file()
        print(f"Loaded {len(self.gesture_labels)} cử chỉ:")
        for gid, gname in self.gesture_labels.items():
            print(f"  {gid}: {gname}")
        
        # Khởi tạo detector
        self.detector = HandLandmarksDetector()
        
        # Load mô hình đã huấn luyện
        print(f"Đang load mô hình: {model_path}")
        checkpoint = torch.load(model_path, map_location='cpu')
        self.model = HandGestureModel(
            input_size=63, 
            num_classes=checkpoint['num_classes']
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        print("Mô hình đã sẵn sàng!")
        
        # Khởi tạo relay controller
        self.simulation = simulation
        if not simulation and port:
            try:
                self.relay = ModbusMaster(port)
            except Exception as e:
                print(f"Lỗi kết nối relay: {e}")
                print("Chuyển sang chế độ mô phỏng")
                self.simulation = True
        else:
            self.simulation = True
        
        if self.simulation:
            print("CHẾ ĐỘ MÔ PHỎNG: Các lệnh relay sẽ được in ra màn hình")
        
        # Trạng thái relay
        self.relay_states = [False, False]  # [relay1, relay2]
        self.last_gesture_time = 0
        self.gesture_cooldown = 1.0  # Chờ 1 giây giữa các lệnh
    
    def execute_gesture_command(self, gesture_id):
        """Thực thi lệnh tương ứng với cử chỉ"""
        current_time = time.time()
        
        # Kiểm tra cooldown
        if current_time - self.last_gesture_time < self.gesture_cooldown:
            return
        
        gesture_name = self.gesture_labels.get(gesture_id, "unknown")
        
        if gesture_id == 1:  # light1_on
            self.relay_states[0] = True
            if self.simulation:
                print("MÔ PHỎNG: Relay 1 BẬT")
            else:
                self.relay.switch_relay_1(True)
        
        elif gesture_id == 2:  # light1_off
            self.relay_states[0] = False
            if self.simulation:
                print("MÔ PHỎNG: Relay 1 TẮT")
            else:
                self.relay.switch_relay_1(False)
        
        elif gesture_id == 3:  # light2_on
            self.relay_states[1] = True
            if self.simulation:
                print("MÔ PHỎNG: Relay 2 BẬT")
            else:
                self.relay.switch_relay_2(True)
        
        elif gesture_id == 4:  # light2_off
            self.relay_states[1] = False
            if self.simulation:
                print("MÔ PHỎNG: Relay 2 TẮT")
            else:
                self.relay.switch_relay_2(False)
        
        elif gesture_id == 5:  # turn_on
            self.relay_states = [True, True]
            if self.simulation:
                print("MÔ PHỎNG: Tất cả relay BẬT")
            else:
                self.relay.all_on()
        
        elif gesture_id == 0:  # turn_off
            self.relay_states = [False, False]
            if self.simulation:
                print("MÔ PHỎNG: Tất cả relay TẮT")
            else:
                self.relay.all_off()
        
        self.last_gesture_time = current_time
        print(f"Thực thi: {gesture_name}")
    
    def run(self):
        """Chạy hệ thống điều khiển real-time"""
        print("\nBắt đầu điều khiển real-time...")
        print("Nhấn 'q' để thoát")
        
        # Khởi tạo camera
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Phát hiện tay
            landmarks, annotated_frame = self.detector.detect_hand(frame)
            
            # Nhận diện cử chỉ nếu có phát hiện tay
            if landmarks:
                # Chuẩn bị input cho mô hình
                input_tensor = torch.tensor(landmarks[0], dtype=torch.float32).unsqueeze(0)
                
                # Dự đoán cử chỉ
                with torch.no_grad():
                    outputs = self.model(input_tensor)
                    probabilities = torch.softmax(outputs, dim=1)
                    confidence, predicted = torch.max(probabilities, 1)
                    
                    # Chỉ thực thi nếu tin cậy cao
                    if confidence.item() > 0.7:
                        gesture_id = predicted.item()
                        gesture_name = self.gesture_labels.get(gesture_id, "unknown")
                        
                        # Hiển thị kết quả
                        cv2.putText(annotated_frame, f"Cu chi: {gesture_name}", 
                                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                        cv2.putText(annotated_frame, f"Tin cay: {confidence.item():.2f}", 
                                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                        
                        # Thực thi lệnh
                        self.execute_gesture_command(gesture_id)
            
            # Hiển thị trạng thái relay
            status = f"Relay 1: {'ON' if self.relay_states[0] else 'OFF'} | "
            status += f"Relay 2: {'ON' if self.relay_states[1] else 'OFF'}"
            cv2.putText(annotated_frame, status, 
                       (10, annotated_frame.shape[0] - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.imshow('Dieu khien relay bang cu chi tay', annotated_frame)
            
            # Nhấn 'q' để thoát
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        # Dọn dẹp
        cap.release()
        cv2.destroyAllWindows()
        if hasattr(self, 'relay') and not self.simulation:
            self.relay.close()
        print("Đã thoát hệ thống")
\end{python}
\end{aivncodebox}

\subsection{Cách chạy ứng dụng chính}

\begin{aivncodebox}
\begin{python}
# Chạy hệ thống điều khiển relay
cd gesture_recognization/

# Chế độ mô phỏng (không cần relay thật)
python -m app.main_controller --simulation

# Chế độ phần cứng thật (cần relay module)
python -m app.main_controller --port COM3

# Với các tùy chọn khác
python -m app.main_controller \
    --model training/models/hand_gesture_model.pth \
    --config config.yaml \
    --simulation
"

# Chế độ thực tế (cần relay module)
python -c "
from relay_controller import RelayGestureController

controller = RelayGestureController(
    model_path='training/models/hand_gesture_model.pth',
    port='COM3',  # Thay bằng port thực tế
    simulation=False
)
controller.run()
"
\end{python}
\end{aivncodebox}

\subsection{Cách sử dụng hệ thống}

\textbf{Các cử chỉ có thể nhận diện:}
\begin{itemize}
    \item \texttt{light1\_on}: Bật relay 1
    \item \texttt{light1\_off}: Tắt relay 1
    \item \texttt{light2\_on}: Bật relay 2
    \item \texttt{light2\_off}: Tắt relay 2
    \item \texttt{turn\_on}: Bật tất cả relay
    \item \texttt{turn\_off}: Tắt tất cả relay
\end{itemize}

\textbf{Hướng dẫn sử dụng:}
\begin{itemize}
    \item Ngồi cách camera 50-80cm
    \item Thực hiện cử chỉ rõ ràng và giữ trong 1-2 giây
    \item Hệ thống sẽ tự động nhận diện và thực thi lệnh
    \item Có thời gian chờ 1 giây giữa các lệnh
    \item Nhấn 'q' để thoát
\end{itemize}

\textbf{Giải thích giao thức Modbus RTU:}
\begin{itemize}
    \item Function Code 05: Write Single Coil (điều khiển relay)
    \item Cấu trúc lệnh: [SlaveID, Function, Address, Value, CRC]
    \item Tốc độ: 9600 baud, 8 data bits, 1 stop bit
    \item Giá trị: 0xFF00 = ON, 0x0000 = OFF
\end{itemize}

% =====================
% KẾT LUẬN VÀ TỔNG KẾT
% =====================

\section{Tổng Kết Dự Án}

\subsection{Quy trình hoàn chỉnh}
Để hoàn thành dự án, bạn cần thực hiện theo thứ tự:

\begin{enumerate}
    \item \textbf{Chuẩn bị:} Cài đặt Python 3.10+ và các thư viện cần thiết theo \texttt{requirements.txt}
    \item \textbf{Thu thập dữ liệu:} Chạy \texttt{python -m processing.data\_collector} để tạo datasets train/val/test
    \item \textbf{Huấn luyện mô hình:} Chạy \texttt{python -m training.gesture\_trainer\_recognizer} để train neural network
    \item \textbf{Chạy ứng dụng:} Sử dụng \texttt{python -m app.main\_controller} để điều khiển relay real-time
\end{enumerate}

\subsection{Các module và file quan trọng}
\begin{itemize}
    \item \texttt{config.yaml}: Cấu hình hệ thống và gesture mapping
    \item \texttt{common/models.py}: Các class chung (HandLandmarksDetector, HandGestureModel)
    \item \texttt{processing/data\_collector.py}: Thu thập dữ liệu gesture
    \item \texttt{training/gesture\_trainer\_recognizer.py}: Huấn luyện mô hình AI
    \item \texttt{app/main\_controller.py}: Ứng dụng chính điều khiển relay
    \item \texttt{hardware/modbus\_controller.py}: Giao tiếp Modbus RTU với relay
\end{itemize}

\subsection{Chế độ sử dụng}
\textbf{Simulation mode (test gesture recognition):}
\begin{itemize}
    \item Chạy: \texttt{cd gesture\_recognization \&\& python -m app.main\_controller --simulation}
    \item LED ảo hiển thị trạng thái relay trên giao diện
    \item Hoàn hảo cho testing và demo mà không cần hardware
\end{itemize}

\textbf{Hardware mode (điều khiển relay thật):}
\begin{itemize}
    \item Chạy: \texttt{cd gesture\_recognization \&\& python -m app.main\_controller --port COM3}
    \item Kết nối với relay module qua Modbus RTU
    \item Áp dụng debounce logic để tránh nhận diện sai
\end{itemize}

\subsection{Khắc phục sự cố thường gặp}
\begin{itemize}
    \item \textbf{Camera không hoạt động:} Kiểm tra driver webcam và quyền truy cập
    \item \textbf{Mô hình nhận diện kém:} Thu thập thêm dữ liệu (>500 samples/gesture) và huấn luyện lại
    \item \textbf{Relay không phản hồi:} Kiểm tra kết nối COM port, baudrate 9600, và slave ID = 1
    \item \textbf{Nhận diện cử chỉ không ổn định:} Điều chỉnh ánh sáng, khoảng cách camera, và ngưỡng confidence
    \item \textbf{Import errors:} Đảm bảo chạy từ thư mục root và sử dụng \texttt{python -m module.file}
\end{itemize}

\subsection{Quy Trình Thu Thập Dữ Liệu}

\textbf{Bước 1: Chuẩn bị môi trường}
\begin{itemize}
    \item Đảm bảo camera hoạt động ổn định
    \item Ánh sáng đều, nền đơn giản không nhiễu
    \item Khoảng cách từ camera 40-80cm
    \item Tay sạch, không đeo nhẫn hoặc trang sức che khuất
\end{itemize}

\textbf{Bước 2: Thu thập cho từng gesture}
\begin{itemize}
    \item Nhấn phím 'a' đến 'f' để chọn gesture theo config (a=0, b=1, ..., f=5)
    \item Nhấn phím lần 2 để bắt đầu/dừng recording
    \item Thực hiện cử chỉ liên tục và đồng đều
    \item Thu thập ít nhất 200-500 samples cho mỗi gesture
    \item Nhấn 's' để lưu ảnh mẫu demonstration
\end{itemize}

\textbf{Bước 3: Phân chia dataset}
\begin{itemize}
    \item Train dataset: 70\% dữ liệu (nhiều samples nhất)
    \item Validation dataset: 15\% dữ liệu
    \item Test dataset: 15\% dữ liệu
    \item Đảm bảo phân bố đều các gesture trong mỗi split
\end{itemize}

\textbf{Bước 4: Kiểm tra chất lượng}
\begin{itemize}
    \item Kiểm tra số lượng samples cho mỗi gesture
    \item Xem lại ảnh mẫu để đảm bảo chất lượng
    \item Kiểm tra file CSV không bị lỗi format
\end{itemize}

\begin{aivncodebox}
\begin{python}
# data_collector.py - Main classes and logic
import os
import csv
import cv2
import os
import numpy as np
from typing import Dict, Tuple, Optional, List
# Import shared classes from common module
from gesture_recognization.common.models import HandLandmarksDetector, label_dict_from_config_file

class HandDatasetWriter:
    """Class for writing landmark data to CSV file"""
    def __init__(self, file_path: str, buffer_size: int = 100):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        self.file_path = file_path
        self.buffer_size = buffer_size
        self.sample_count = 0
        self.buffer_count = 0
        self.csv_file = open(self.file_path, 'w', newline="", encoding="utf-8")
        self.file_writer = csv.writer(
            self.csv_file,
            delimiter=',',
            quotechar='|',
            quoting=csv.QUOTE_MINIMAL
        )
        header = []
        for i in range(21):
            header.extend([f'x{i}', f'y{i}', f'z{i}'])
        header.append('label')
        self.file_writer.writerow(header)
        print(f'Initialized csv file at {self.file_path}')
    def add(self, hand_landmarks: List[float], label: int):
        if len(hand_landmarks) != 63:
            raise ValueError(f"Incorrect number of landmarks: {len(hand_landmarks)} (expected 63)")
        flattened_landmarks = np.array(hand_landmarks).flatten().tolist()
        row = flattened_landmarks + [label]
        self.file_writer.writerow(row)
        self.sample_count += 1
        self.buffer_count += 1
        if self.buffer_count >= self.buffer_size:
            self.flush()
    def flush(self):
        self.csv_file.flush()
        self.buffer_count = 0
    def close(self):
        self.flush()
        self.csv_file.close()
        print(f"Closed file {self.file_path}. Total samples: {self.sample_count}")

class GestureDataCollector:
    """Main class for collecting hand gesture data"""
    def __init__(self, config_path: str = "../config.yaml"):
        self.config_path = config_path
        self.gesture_labels = label_dict_from_config_file(config_path)
        self.detector = HandLandmarksDetector()
    def collect_data(self, mode: str, data_path: str = "./data", img_path: str = "./sample_images", resolution: Tuple[int, int] = (1280, 720)):
        os.makedirs(data_path, exist_ok=True)
        os.makedirs(img_path, exist_ok=True)
        csv_file = os.path.join(data_path, f"landmarks_{mode}.csv")
        writer = HandDatasetWriter(csv_file)
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, resolution[0])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, resolution[1])
        current_label = None
        is_recording = False
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Cannot read frame from camera!")
                break
            landmarks, annotated_frame = self.detector.detect_hand(frame)
            if is_recording and landmarks and current_label is not None:
                writer.add(landmarks[0], current_label)
                cv2.putText(annotated_frame, "RECORDING...", (10, 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
            cv2.imshow(f'Hand Gesture Data Collection - {mode.upper()}', annotated_frame)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key != 255:
                char = chr(key)
                # ...existing code for key handling...
        cap.release()
        cv2.destroyAllWindows()
        writer.close()
        print(f"Finished collecting data for {mode}")

def main():
    collector = GestureDataCollector()
    datasets = ["train", "val", "test"]
    for dataset in datasets:
        input(f"\nPress Enter to start collecting {dataset} data...")
        collector.collect_data(dataset)
        print(f"Finished collecting {dataset} data\n")
    print("Finished collecting all data!")

if __name__ == "__main__":
    main()
\end{python}
\end{aivncodebox}

\subsubsection{Configuration Loading Functions}

\begin{aivncodebox}
\begin{python}
import yaml
from typing import Dict

def load_config(config_path: str = "config.yaml") -> Dict:
    """Load configuration from YAML file"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        print(f"Error loading config: {e}")
        raise

def get_gesture_mapping(config_path: str = "config.yaml") -> Dict[int, str]:
    """Get gesture mapping from config"""
    config = load_config(config_path)
    return config.get('gestures', {})

def is_handsign_character(char: str) -> bool:
    """Check if character is valid for gesture mapping (a-p or space)"""
    return ord('a') <= ord(char) <= ord('p') or char == " "
\end{python}
\end{aivncodebox}

\subsection{Main Data Collection Logic}

\subsubsection{Function run() - Core Collection Process}

\begin{aivncodebox}
\begin{python}
def run_data_collection(data_path: str, image_path: str, mode: str, 
                       resolution: tuple = (1280, 720)) -> None:
    """
    Run the data collection process for a dataset (train/val/test)
    
    Args:
        data_path: Directory to save CSV files
        image_path: Directory to save sample images  
        mode: Dataset mode ('train', 'val', 'test')
        resolution: Camera resolution (width, height)
    """
    # Create output directories
    os.makedirs(data_path, exist_ok=True)
    os.makedirs(image_path, exist_ok=True)
    
    # Load gesture configuration
    gesture_mapping = get_gesture_mapping()
    
    # Initialize components
    detector = HandLandmarksDetector()
    csv_path = os.path.join(data_path, f"landmarks_{mode}.csv")
    writer = HandDatasetWriter(csv_path)
    
    # Initialize camera
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, resolution[0])
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, resolution[1])
    
    # State variables
    current_gesture_id = None
    is_recording = False
    frame_count = 0
    
    print(f"=== DATA COLLECTION - {mode.upper()} MODE ===")
    print("Controls:")
    print("- Keys 'a'-'p': Select gesture (ID 0-15)")
    print("- Space: Select gesture ID 0 (turn_off)")  
    print("- Press key 1st time: Select gesture")
    print("- Press key 2nd time: Start/stop recording")
    print("- Key 's': Save sample image")
    print("- Key 'q': Quit")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Cannot read from camera!")
            break
        
        # Detect hand landmarks
        hands, annotated_frame = detector.detect_hand(frame)
        
        # Display UI information
        display_frame = annotated_frame.copy()
        
        # Status text
        status_text = f"Mode: {mode.upper()}"
        if current_gesture_id is not None:
            gesture_name = gesture_mapping.get(current_gesture_id, "Unknown")
            status_text += f" | Gesture: {gesture_name} (ID: {current_gesture_id})"
        status_text += f" | Recording: {'ON' if is_recording else 'OFF'}"
        
        cv2.putText(display_frame, status_text, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # Recording indicator and data collection
        if is_recording and hands and current_gesture_id is not None:
            writer.add(hands[0], current_gesture_id)
            frame_count += 1
            
            # Recording indicator
            cv2.putText(display_frame, f"RECORDING... Frames: {frame_count}", 
                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
            # Progress circle
            cv2.circle(display_frame, (50, 120), 20, (0, 0, 255), -1)
        
        # Hand detection indicator
        if hands:
            cv2.putText(display_frame, "Hand Detected", (10, display_frame.shape[0] - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv2.imshow('Hand Gesture Data Collection', display_frame)
        
        # Keyboard input handling
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            break
        elif key == ord('s') and hands:
            # Save sample image
            if current_gesture_id is not None:
                gesture_name = gesture_mapping.get(current_gesture_id, "unknown")
                img_filename = f"{gesture_name}_{mode}_sample.jpg"
                img_path = os.path.join(image_path, img_filename)
                cv2.imwrite(img_path, display_frame)
                print(f"Saved sample: {img_filename}")
        elif is_handsign_character(chr(key)) if key < 256 else False:
            # Handle gesture selection
            if chr(key) == ' ':
                new_gesture_id = 0  # Space = turn_off
            else:
                new_gesture_id = ord(chr(key)) - ord('a') + 1
            
            if new_gesture_id in gesture_mapping:
                if current_gesture_id == new_gesture_id:
                    # Toggle recording for same gesture
                    is_recording = not is_recording
                    if is_recording:
                        frame_count = 0
                        print(f"Started recording: {gesture_mapping[new_gesture_id]}")
                    else:
                        print(f"Stopped recording. Collected {frame_count} frames")
                else:
                    # Select new gesture
                    current_gesture_id = new_gesture_id
                    is_recording = False
                    frame_count = 0
                    print(f"Selected gesture: {gesture_mapping[new_gesture_id]}")
            else:
                print(f"Invalid gesture ID: {new_gesture_id}")
    
    # Cleanup
    cap.release()
    cv2.destroyAllWindows()
    writer.close()
    print(f"Data collection completed for {mode}")
\end{python}
\end{aivncodebox}
\subsubsection{Main Function và Usage}

\begin{aivncodebox}
\begin{python}
def main():
    """Main function cho data collection"""
    print("=== GESTURE DATA COLLECTION SYSTEM ===")
    
    # Load configuration
    try:
        gesture_mapping = get_gesture_mapping()
        print(f"Loaded {len(gesture_mapping)} gestures from config.yaml")
        for gid, gname in gesture_mapping.items():
            print(f"  {gid}: {gname}")
    except Exception as e:
        print(f"Error loading config: {e}")
        return
    
    # Setup paths
    data_path = "./data"
    image_path = "./sample_images"
    
    # Collection workflow
    datasets = ["train", "val", "test"]
    
    for dataset in datasets:
        print(f"\n{'='*50}")
        print(f"Starting {dataset.upper()} data collection...")
        print(f"{'='*50}")
        
        input(f"Press Enter to start {dataset} collection...")
        
        try:
            run_data_collection(data_path, image_path, dataset)
        except Exception as e:
            print(f"Error during {dataset} collection: {e}")
            continue
   
if __name__ == "__main__":
    main()
\end{python}
\end{aivncodebox}

\subsection{Cách sử dụng module thu thập dữ liệu}

\subsubsection{Chạy từ main.py}

\begin{aivncodebox}
\begin{python}
# Di chuyển vào thư mục dự án
cd gesture_recognization/

# Chạy main menu
python main.py

# Chọn option 1: Data Collection
\end{python}
\end{aivncodebox}

\subsubsection{Chạy trực tiếp module}

\begin{aivncodebox}
\begin{python}
# Chạy trực tiếp module thu thập dữ liệu
cd gesture_recognization/
python data_collector.py

# Hoặc test compatibility
python test_compatibility.py
\end{python}
\end{aivncodebox}

\subsubsection{Workflow Thu thập Dữ liệu}

\textbf{Bước 1: Chuẩn bị}
\begin{itemize}
    \item Đảm bảo camera hoạt động tốt
    \item Môi trường ánh sáng đều, nền đơn giản
    \item Khoảng cách camera 40-80cm
\end{itemize}

\textbf{Bước 2: Thu thập theo từng gesture}
\begin{itemize}
    \item Nhấn phím 'a' đến 'p' để chọn gesture (theo config.yaml)
    \item Nhấn phím lần 2 để bắt đầu recording
    \item Thực hiện cử chỉ liên tục cho đến khi đủ samples
    \item Nhấn phím lần 3 để dừng recording
\end{itemize}

\textbf{Bước 3: Lưu sample images}
\begin{itemize}
    \item Nhấn 's' để lưu ảnh mẫu cho gesture hiện tại
    \item Ảnh sẽ được lưu vào thư mục sample\_images/
\end{itemize}

\textbf{Bước 4: Lặp lại cho tất cả datasets}
\begin{itemize}
    \item Train dataset: 70\% dữ liệu (nhiều samples nhất)
    \item Validation dataset: 15\% dữ liệu  
    \item Test dataset: 15\% dữ liệu
\end{itemize}

\subsection{Output và Kiểm tra Chất lượng}

\subsubsection{Cấu trúc Output}

\begin{aivncodebox}
\begin{python}
# CSV Structure: 64 columns
# [x0, y0, z0, x1, y1, z1, ..., x20, y20, z20, label]
# 
# Ví dụ một row trong landmarks_train.csv:
# 0.5234,0.3421,0.0012,...,0.7891,0.4567,0.0089,2
#
# Với:
# - 63 columns đầu: tọa độ (x,y,z) của 21 hand landmarks
# - Column cuối: label ID (0-6 theo config.yaml)
\end{python}
\end{aivncodebox}

\subsubsection{Kiểm tra Chất lượng Dữ liệu}

\begin{aivncodebox}
\begin{python}
import pandas as pd

def check_dataset_quality(csv_path: str):
    """Kiểm tra chất lượng dataset"""
    df = pd.read_csv(csv_path)
    
    print(f"Dataset: {csv_path}")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.shape[1] - 1}")  # Trừ label column
    
    # Check distribution
    print("\nGesture distribution:")
    gesture_counts = df['label'].value_counts().sort_index()
    for label_id, count in gesture_counts.items():
        print(f"  ID {label_id}: {count} samples")
    
    # Check for missing values
    missing = df.isnull().sum().sum()
    print(f"\nMissing values: {missing}")
    
    # Check value ranges
    feature_cols = [col for col in df.columns if col != 'label']
    print(f"\nFeature ranges:")
    print(f"  Min: {df[feature_cols].min().min():.4f}")
    print(f"  Max: {df[feature_cols].max().max():.4f}")

# Usage
check_dataset_quality("./data/landmarks_train.csv")
\end{python}
\end{aivncodebox}

% =====================
% STEP 2: MODEL TRAINING MODULE
% =====================

\section{Step 2: Huấn Luyện và Nhận Diện Model}

Step 2 chịu trách nhiệm huấn luyện neural network để phân loại hand gestures từ landmarks data và thực hiện nhận diện real-time. Module này sử dụng PyTorch để xây dựng và huấn luyện mô hình deep learning.

\subsection{Dataset và Training Classes}

\begin{aivncodebox}
\begin{python}
# training/gesture_trainer_recognizer.py - Training implementation
import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Tuple
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, classification_report
from gesture_recognization.common.models import HandGestureModel, label_dict_from_config_file

class HandGestureDataset(Dataset):
    """PyTorch Dataset for hand gesture landmarks"""
    def __init__(self, csv_path: str):
        self.data = pd.read_csv(csv_path)
        print(f"Loaded dataset: {csv_path}")
        print(f"Shape: {self.data.shape}")
        
        # Features: all columns except last (label)
        self.features = self.data.iloc[:, :-1].values.astype(np.float32)
        # Labels: last column
        self.labels = self.data.iloc[:, -1].values.astype(np.int64)
        
        # Print dataset statistics
        unique_labels = np.unique(self.labels)
        print(f"Classes: {unique_labels}")
        for label in unique_labels:
            count = np.sum(self.labels == label)
            print(f"  Class {label}: {count} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])

class EarlyStopper:
    """Early stopping to prevent overfitting"""
    def __init__(self, patience=30, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def early_stop(self, current_loss):
        """Check if training should stop early"""
        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

class HandGestureTrainer:
    """Main training controller"""
    def __init__(self, config_path: str = "../config.yaml"):
        self.config_path = config_path
        self.gesture_labels = label_dict_from_config_file(config_path)
        self.num_classes = len(self.gesture_labels)
        print(f"Training for {self.num_classes} gesture classes:")
        for gid, gname in self.gesture_labels.items():
            print(f"  {gid}: {gname}")
    
    def prepare_data(self, data_folder: str = "data", batch_size: int = 32):
        """Load and prepare training/validation datasets"""
        train_path = os.path.join(data_folder, "landmarks_train.csv")
        val_path = os.path.join(data_folder, "landmarks_val.csv")
        
        # Check if data files exist
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"Training data not found: {train_path}")
        if not os.path.exists(val_path):
            raise FileNotFoundError(f"Validation data not found: {val_path}")
        
        # Create datasets
        train_dataset = HandGestureDataset(train_path)
        val_dataset = HandGestureDataset(val_path)
        
        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader
    
    def train_model(self, train_loader, val_loader, epochs=200, learning_rate=0.001):
        """Train the hand gesture recognition model"""
        # Initialize model
        model = HandGestureModel(input_size=63, num_classes=self.num_classes)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        early_stopper = EarlyStopper(patience=30, min_delta=0.001)
        
        # Training tracking
        best_val_loss = float('inf')
        best_model_state = None
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        
        print(f"\nStarting training for {epochs} epochs...")
        print("=" * 60)
        
        for epoch in range(epochs):
            # Training phase
            model.train()
            train_loss = 0.0
            train_predictions = []
            train_targets = []
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                # Get predictions for accuracy calculation
                _, predicted = torch.max(output.data, 1)
                train_predictions.extend(predicted.cpu().numpy())
                train_targets.extend(target.cpu().numpy())
            
            # Validation phase
            model.eval()
            val_loss = 0.0
            val_predictions = []
            val_targets = []
            
            with torch.no_grad():
                for data, target in val_loader:
                    output = model(data)
                    val_loss += criterion(output, target).item()
                    
                    _, predicted = torch.max(output.data, 1)
                    val_predictions.extend(predicted.cpu().numpy())
                    val_targets.extend(target.cpu().numpy())
            
            # Calculate metrics
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_accuracy = accuracy_score(train_targets, train_predictions)
            val_accuracy = accuracy_score(val_targets, val_predictions)
            
            # Store metrics
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            train_accuracies.append(train_accuracy)
            val_accuracies.append(val_accuracy)
            
            # Print progress
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{epochs}")
                print(f"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}")
                print(f"  Val Loss:   {avg_val_loss:.4f}, Val Acc:   {val_accuracy:.4f}")
            
            # Save best model
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = model.state_dict().copy()
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    print(f"  -> New best model! (Val Loss: {best_val_loss:.4f})")
            
            # Early stopping check
            if early_stopper.early_stop(avg_val_loss):
                print(f"\nEarly stopping at epoch {epoch+1}")
                print(f"Best validation loss: {best_val_loss:.4f}")
                break
        
        # Load best model state
        if best_model_state:
            model.load_state_dict(best_model_state)
            print(f"\nLoaded best model (Val Loss: {best_val_loss:.4f})")
        
        return model, {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies
        }
    
    def evaluate_model(self, model, test_data_path: str):
        """Evaluate model on test dataset"""
        if not os.path.exists(test_data_path):
            print("Test data not found, skipping evaluation")
            return
        
        print(f"\nEvaluating on test data: {test_data_path}")
        test_dataset = HandGestureDataset(test_data_path)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        model.eval()
        test_predictions = []
        test_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                test_predictions.extend(predicted.cpu().numpy())
                test_targets.extend(target.cpu().numpy())
        
        # Calculate test accuracy
        test_accuracy = accuracy_score(test_targets, test_predictions)
        print(f"Test Accuracy: {test_accuracy:.4f}")
        
        # Print detailed classification report
        target_names = [self.gesture_labels[i] for i in sorted(self.gesture_labels.keys())]
        report = classification_report(test_targets, test_predictions, 
                                     target_names=target_names, zero_division=0)
        print("\nDetailed Classification Report:")
        print(report)
    
    def save_model(self, model, save_path="./models"):
        """Save trained model with metadata"""
        os.makedirs(save_path, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f"hand_gesture_model_{timestamp}.pth"
        model_path = os.path.join(save_path, model_filename)
        
        # Save model with metadata
        torch.save({
            'model_state_dict': model.state_dict(),
            'gesture_labels': self.gesture_labels,
            'num_classes': self.num_classes,
            'input_size': 63,
            'timestamp': timestamp
        }, model_path)
        
        # Also save as default model for easy loading
        default_path = os.path.join(save_path, "hand_gesture_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'gesture_labels': self.gesture_labels,
            'num_classes': self.num_classes,
            'input_size': 63,
            'timestamp': timestamp
        }, default_path)
        
        print(f"\nModel saved:")
        print(f"  Timestamped: {model_path}")
        print(f"  Default:     {default_path}")
        return model_path

def main():
    """Main training workflow"""
    print("=== HAND GESTURE MODEL TRAINING ===")
    
    try:
        # Initialize trainer
        trainer = HandGestureTrainer()
        
        # Prepare data
        print("\nPreparing training data...")
        train_loader, val_loader = trainer.prepare_data()
        
        # Train model
        print("\nStarting model training...")
        model, training_history = trainer.train_model(train_loader, val_loader)
        
        # Evaluate on test set
        test_path = "data/landmarks_test.csv"
        trainer.evaluate_model(model, test_path)
        
        # Save model
        model_path = trainer.save_model(model)
        
        print("\n=== TRAINING COMPLETED SUCCESSFULLY ===")
        print(f"Model ready for use in Step 3: {model_path}")
        
    except Exception as e:
        print(f"Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
\end{python}
\end{aivncodebox}

\subsection{Quá Trình Huấn Luyện Model}

\textbf{Bước 1: Chuẩn bị dữ liệu}
\begin{itemize}
    \item Load data từ các file CSV được tạo ở Step 1
    \item Kiểm tra chất lượng và phân bố dữ liệu
    \item Chia thành train/validation/test sets
    \item Chuẩn hóa và chuyển đổi sang PyTorch tensors
\end{itemize}

\textbf{Bước 2: Thiết kế architecture}
\begin{itemize}
    \item Neural network với 4 fully-connected layers
    \item Input: 63 features (21 landmarks × 3 coordinates)
    \item Hidden layers: 256 → 128 → 64 neurons
    \item Output: số lượng gesture classes (6 classes)
    \item Activation: ReLU, Dropout để tránh overfitting
\end{itemize}

\textbf{Bước 3: Training process}
\begin{itemize}
    \item Optimizer: Adam với learning rate 0.001
    \item Loss function: CrossEntropyLoss
    \item Batch size: 32 samples
    \item Early stopping với patience=30 epochs
    \item Theo dõi accuracy và loss trên train/validation sets
\end{itemize}

\textbf{Bước 4: Evaluation và testing}
\begin{itemize}
    \item Đánh giá model trên test set
    \item Tính accuracy và classification report chi tiết
    \item Lưu model tốt nhất với metadata
    \item Chuẩn bị cho deployment ở Step 3
\end{itemize}

\begin{aivncodebox}
\begin{python}
# gesture_trainer_recognizer.py - Main classes and logic
import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import yaml
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from torch.utils.data import Dataset, DataLoader
from torchmetrics import Accuracy

class HandGestureDataset(Dataset):
    """Dataset class for hand gesture landmarks"""
    def __init__(self, csv_path: str):
        self.data = pd.read_csv(csv_path)
        self.features = self.data.iloc[:, :-1].values.astype(np.float32)
        self.labels = self.data.iloc[:, -1].values.astype(np.int64)
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])

class HandGestureModel(nn.Module):
    """Neural Network for hand gesture recognition"""
    def __init__(self, input_size=63, num_classes=6):
        super(HandGestureModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size,256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256,128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128,64),
            nn.Linear(64,num_classes)
        )
    def forward(self, x):
        return self.network(x)
    def get_logits(self, x):
        with torch.no_grad():
            outputs = self.forward(x)
            return torch.argmax(outputs, dim=1)

class EarlyStopper:
    """Early stopping to prevent overfitting"""
    def __init__(self, patience=30, min_delta=0.01):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
    def early_stop(self, current_loss):
        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False

class HandGestureTrainer:
    """Model training class"""
    def __init__(self, config_path: str = "../config.yaml"):
        self.config_path = config_path
        self.gesture_labels = self._load_gesture_config()
        self.num_classes = len(self.gesture_labels)
    def _load_gesture_config(self) -> Dict[int, str]:
        try:
            with open(self.config_path, 'r', encoding='utf-8') as file:
                config = yaml.safe_load(file)
                return config.get('gestures', {})
        except FileNotFoundError:
            print(f"Config file not found: {self.config_path}")
            return {}
    def prepare_data(self, data_folder: str = "data"):
        train_path = os.path.join(data_folder, "landmarks_train.csv")
        val_path = os.path.join(data_folder, "landmarks_val.csv")
        if not os.path.exists(train_path) or not os.path.exists(val_path):
            raise FileNotFoundError("Data files not found. Please run Step 1 first!")
        train_dataset = HandGestureDataset(train_path)
        val_dataset = HandGestureDataset(val_path)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        return train_loader, val_loader
    def train_model(self, train_loader, val_loader, epochs=200):
        model = HandGestureModel(num_classes=self.num_classes)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        early_stopper = EarlyStopper()
        best_val_loss = float('inf')
        best_model_state = None
        print(f"Training started with {self.num_classes} gesture classes...")
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0
            train_acc = Accuracy(num_classes=self.num_classes, task='multiclass')
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
                train_acc.update(model.get_logits(data), target)
            model.eval()
            val_loss = 0.0
            val_acc = Accuracy(num_classes=self.num_classes, task='multiclass')
            with torch.no_grad():
                for data, target in val_loader:
                    output = model(data)
                    val_loss += criterion(output, target).item()
                    val_acc.update(model.get_logits(data), target)
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_accuracy = train_acc.compute().item()
            val_accuracy = val_acc.compute().item()
            print(f"Epoch {epoch+1}/{epochs}:")
            print(f"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}")
            print(f"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = model.state_dict().copy()
                print(f"  -> New best model!")
            if early_stopper.early_stop(avg_val_loss):
                print(f"Early stopping at epoch {epoch+1}")
                break
        if best_model_state:
            model.load_state_dict(best_model_state)
        return model
    def save_model(self, model, save_path="./models"):
        os.makedirs(save_path, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = os.path.join(save_path, f"hand_gesture_model_{timestamp}.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'gesture_labels': self.gesture_labels,
            'num_classes': self.num_classes
        }, model_path)
        print(f"Model saved at: {model_path}")
        return model_path

def main():
    print("=== STEP 2: TRAINING AND RECOGNITION ===")
    print("Training model")
    trainer = HandGestureTrainer()
    train_loader, val_loader = trainer.prepare_data()
    model = trainer.train_model(train_loader, val_loader)
    model_path = trainer.save_model(model)
    print(f"Training complete! Model saved at: {model_path}")

if __name__ == "__main__":
    main()
\end{python}
\end{aivncodebox}

% =====================
% STEP 3: REAL-TIME RELAY CONTROL MODULE
% =====================

\section{Step 3: Điều Khiển Relay Thời Gian Thực}

Step 3 là module cuối cùng, chịu trách nhiệm sử dụng model đã train để nhận diện cử chỉ tay real-time và điều khiển relay hardware thông qua giao thức Modbus RTU. Module này kết hợp computer vision, AI và hardware control.

\subsection{Hardware Communication Classes}

\begin{aivncodebox}
\begin{python}
# app/main_controller.py - Real-time relay control implementation
import cv2
import time
import torch
import numpy as np
import serial
import argparse
from gesture_recognization.common.models import HandGestureModel, HandLandmarksDetector, label_dict_from_config_file

# Modbus RTU commands for relay control (9600 baud standard)
# Structure: [Slave_ID, Function_Code, Address_Hi, Address_Lo, Value_Hi, Value_Lo, CRC_Lo, CRC_Hi]
# Function 05 (0x05): Write Single Coil
# Value: 0xFF00 = ON, 0x0000 = OFF

RELAY1_ON =  [1, 5, 0, 0, 0xFF, 0, 0x8C, 0x3A]  # Turn ON relay 1
RELAY1_OFF = [1, 5, 0, 0, 0,    0, 0xCD, 0xCA]  # Turn OFF relay 1

RELAY2_ON =  [1, 5, 0, 1, 0xFF, 0, 0xDD, 0xFA]  # Turn ON relay 2
RELAY2_OFF = [1, 5, 0, 1, 0,    0, 0x9C, 0x0A]  # Turn OFF relay 2

RELAY3_ON =  [1, 5, 0, 2, 0xFF, 0, 0x2D, 0xFA]  # Turn ON relay 3
RELAY3_OFF = [1, 5, 0, 2, 0,    0, 0x6C, 0x0A]  # Turn OFF relay 3

class ModbusMaster:
    """Simple Modbus relay controller for Windows (9600 baud only)"""
    def __init__(self, port):
        self.ser = serial.Serial(port=port, baudrate=9600, timeout=2)
        print(f"Connected to {port} at 9600 baud")
    
    def switch_actuator_1(self, state):
        """Control relay 1"""
        cmd = RELAY1_ON if state else RELAY1_OFF
        self.ser.write(bytearray(cmd))
    
    def switch_actuator_2(self, state):
        """Control relay 2"""
        cmd = RELAY2_ON if state else RELAY2_OFF
        self.ser.write(bytearray(cmd))
    
    def switch_actuator_3(self, state):
        """Control relay 3"""
        cmd = RELAY3_ON if state else RELAY3_OFF
        self.ser.write(bytearray(cmd))
    
    def all_on(self):
        """Turn all relays ON"""
        self.switch_actuator_1(True)
        self.switch_actuator_2(True)
        self.switch_actuator_3(True)
    
    def all_off(self):
        """Turn all relays OFF"""
        self.switch_actuator_1(False)
        self.switch_actuator_2(False)
        self.switch_actuator_3(False)
    
    def close(self):
        """Close serial connection"""
        self.ser.close()

class RelayGestureControl:
    """Main controller for real-time gesture-based relay control"""
    def __init__(self, model_path, config_path="../config.yaml", 
                 resolution=(1280, 720), port=None, simulation=False):
        self.resolution = resolution
        self.width, self.height = resolution
        self.port = port
        self.simulation = simulation
        
        # Initialize gesture detection
        self.detector = HandLandmarksDetector()
        self.status_text = None
        
        # Load gesture configuration
        self.signs = label_dict_from_config_file(config_path)
        print(f"Loaded {len(self.signs)} gesture classes:")
        for gid, gname in self.signs.items():
            print(f"  {gid}: {gname}")
        
        # Load trained model
        print(f"Loading model from: {model_path}")
        try:
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
            model_dict = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint
            num_classes = len(self.signs)
            self.classifier = HandGestureModel(input_size=63, num_classes=num_classes)
            self.classifier.load_state_dict(model_dict)
            self.classifier.eval()
            print("Model loaded successfully")
        except Exception as e:
            print(f"Error loading model: {e}")
            raise
        
        # Initialize hardware controller
        self.controller = None
        if not simulation and port:
            try:
                self.controller = ModbusMaster(port=port)
                print("Hardware controller initialized")
            except Exception as e:
                print(f"Error connecting to hardware: {e}")
                print("Automatically switching to SIMULATION mode...")
                self.simulation = True
                self.controller = None
        else:
            self.simulation = True
        
        if self.simulation:
            print("SIMULATION MODE: All relay commands will be simulated")
        
        # Control state
        self.light1 = False
        self.light2 = False
        self.light3 = False
        self.last_command_time = time.time()
        self.command_debounce = 1.0  # Minimum time between commands (seconds)
        
        print("Relay Gesture Control system initialized")
    
    def _control_relay(self, relay_num, state):
        """Control individual relay"""
        if self.simulation:
            action = "ON" if state else "OFF"
            print(f"SIMULATION: Relay {relay_num} -> {action}")
            return
        
        try:
            if relay_num == 1:
                self.controller.switch_actuator_1(state)
            elif relay_num == 2:
                self.controller.switch_actuator_2(state)
            elif relay_num == 3:
                self.controller.switch_actuator_3(state)
            
            action = "ON" if state else "OFF"
            print(f"Relay {relay_num} -> {action}")
        except Exception as e:
            print(f"Error controlling relay {relay_num}: {e}")
    
    def _control_all_relays(self, state):
        """Control all relays simultaneously"""
        if self.simulation:
            action = "ON" if state else "OFF"
            print(f"SIMULATION: ALL RELAYS -> {action}")
            return
        
        try:
            if state:
                self.controller.all_on()
            else:
                self.controller.all_off()
            
            action = "ON" if state else "OFF"
            print(f"ALL RELAYS -> {action}")
        except Exception as e:
            print(f"Error controlling all relays: {e}")
    
    def run(self):
        """Main control loop"""
        # Initialize camera
        cam = cv2.VideoCapture(0)
        cam.set(3, self.width)
        cam.set(4, self.height)
        
        if not cam.isOpened():
            print("Error: Could not open camera")
            return
        
        print("\n" + "=" * 50)
        print("RELAY GESTURE CONTROL SYSTEM - RUNNING")
        print("=" * 50)
        print("Controls:")
        print("- 'q': Quit application")
        print("- 'r': Reset all relays to OFF")
        print(f"- Gestures: {list(self.signs.values())}")
        print("=" * 50)
        
        try:
            while cam.isOpened():
                ret, frame = cam.read()
                if not ret:
                    print("Error: Could not read frame from camera")
                    break
                
                # Detect hand landmarks
                hand, img = self.detector.detect_hand(frame)
                
                if hand:
                    # Process gesture recognition
                    with torch.no_grad():
                        hand_landmark = torch.from_numpy(
                            np.array(hand[0], dtype=np.float32).reshape(1, -1)
                        )
                        class_number = self.classifier.predict(hand_landmark, threshold=0.7).item()
                        
                        if class_number != -1:
                            self.status_text = self.signs[class_number]
                            current_time = time.time()
                            
                            # Command debouncing to prevent rapid switching
                            if current_time - self.last_command_time >= self.command_debounce:
                                self.last_command_time = current_time
                                
                                # Execute gesture commands
                                if self.status_text == "light1_on" and not self.light1:
                                    self.light1 = True
                                    self._control_relay(1, True)
                                elif self.status_text == "light1_off" and self.light1:
                                    self.light1 = False
                                    self._control_relay(1, False)
                                elif self.status_text == "light2_on" and not self.light2:
                                    self.light2 = True
                                    self._control_relay(2, True)
                                elif self.status_text == "light2_off" and self.light2:
                                    self.light2 = False
                                    self._control_relay(2, False)
                                elif self.status_text == "turn_on" and not (self.light1 and self.light2 and self.light3):
                                    self.light1 = self.light2 = self.light3 = True
                                    self._control_all_relays(True)
                                elif self.status_text == "turn_off" and (self.light1 or self.light2 or self.light3):
                                    self.light1 = self.light2 = self.light3 = False
                                    self._control_all_relays(False)
                        else:
                            self.status_text = "undefined gesture"
                else:
                    self.status_text = None
                
                # Display UI information
                if self.status_text:
                    cv2.putText(img, f"Gesture: {self.status_text}", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)
                
                # Display relay status
                status_info = f"Relay 1: {'ON' if self.light1 else 'OFF'} | " \
                             f"Relay 2: {'ON' if self.light2 else 'OFF'} | " \
                             f"Relay 3: {'ON' if self.light3 else 'OFF'}"
                cv2.putText(img, status_info, (10, img.shape[0] - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)
                
                # Display mode indicator
                mode_text = "SIMULATION" if self.simulation else "HARDWARE"
                cv2.putText(img, mode_text, (img.shape[1] - 150, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)
                
                cv2.imshow("Relay Gesture Control", img)
                
                # Handle keyboard input
                key = cv2.waitKey(1)
                if key == ord("q"):
                    break
                elif key == ord("r"):
                    # Reset all relays
                    self.light1 = self.light2 = self.light3 = False
                    self._control_all_relays(False)
                    print("All relays reset to OFF")
        
        except Exception as e:
            print(f"Error in main control loop: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # Cleanup
            if cam.isOpened():
                cam.release()
            cv2.destroyAllWindows()
            
            try:
                # Safety: Turn off all relays on exit
                self._control_all_relays(False)
                if self.controller:
                    self.controller.close()
                
                if self.simulation:
                    print("Simulation ended - All virtual relays turned OFF")
                else:
                    print("All relays turned OFF - Hardware connection closed")
            except:
                pass

def main():
    """Main function with argument parsing"""
    parser = argparse.ArgumentParser(description="Hand Gesture Relay Control System")
    parser.add_argument("--model", "-m", 
                       default="training/models/hand_gesture_model.pth",
                       help="Path to trained model file")
    parser.add_argument("--config", "-c", 
                       default="../config.yaml",
                       help="Path to configuration file")
    parser.add_argument("--resolution", "-r", 
                       default="1280x720",
                       help="Camera resolution (WIDTHxHEIGHT)")
    parser.add_argument("--port", "-p", 
                       help="COM port for relay (e.g., COM3, COM4)")
    parser.add_argument("--simulation", "-s", 
                       action="store_true",
                       help="Run in simulation mode without hardware")
    
    args = parser.parse_args()
    
    # Parse resolution
    try:
        width, height = map(int, args.resolution.split('x'))
        resolution = (width, height)
    except:
        print(f"Invalid resolution format: {args.resolution}. Using default 1280x720")
        resolution = (1280, 720)
    
    try:
        # Initialize control system
        control_system = RelayGestureControl(
            model_path=args.model,
            config_path=args.config,
            resolution=resolution,
            port=args.port,
            simulation=args.simulation
        )
        
        # Start control loop
        control_system.run()
        
    except KeyboardInterrupt:
        print("\nExiting by user request")
    except Exception as e:
        print(f"Error: {e}")
        if not args.simulation:
            print("Try running with --simulation flag to test gesture recognition")
            print("Command: python relay_controller.py --simulation")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
\end{python}
\end{aivncodebox}

\subsection{Hardware Setup và Configuration}

\textbf{Bước 1: Kết nối hardware}
\begin{itemize}
    \item Relay module 3-channel với Modbus RTU support
    \item USB-to-Serial adapter (CH340 chip) để kết nối máy tính với relay
    \item Cáp kết nối phù hợp (USB-A to mini/micro USB)
    \item Nguồn điện 12V DC cho relay module
    \item Thiết bị electrical cần điều khiển (đèn LED, motor, etc.)
\end{itemize}

\textbf{Bước 2: Cấu hình communication}
\begin{itemize}
    \item Baudrate: 9600 bps (chuẩn Modbus RTU)
    \item Data bits: 8, Stop bits: 1, Parity: None
    \item Slave ID của relay module: 1 (default)
    \item Function code: 05 (Write Single Coil)
    \item Kiểm tra COM port trong Device Manager (Windows)
\end{itemize}

\textbf{Bước 3: Testing và validation}
\begin{itemize}
    \item Chạy ở simulation mode trước để test gesture recognition
    \item Kiểm tra kết nối serial với relay hardware
    \item Test manual control từng relay riêng lẻ
    \item Verify gesture-to-relay mapping hoạt động chính xác
\end{itemize}

\subsection{Sử Dụng System}

\textbf{Chạy ở simulation mode (test gesture recognition):}
\begin{aivncodebox}
\begin{python}
cd gesture_recognization/
python -m app.main_controller --simulation
\end{python}
\end{aivncodebox}

\textbf{Chạy với hardware thực tế:}
\begin{aivncodebox}
\begin{python}
cd gesture_recognization/
python -m app.main_controller --port COM3
\end{python}
\end{aivncodebox}

\textbf{Gesture mapping (theo config.yaml):}
\begin{itemize}
    \item \texttt{light1\_on}: Bật relay 1
    \item \texttt{light1\_off}: Tắt relay 1
    \item \texttt{light2\_on}: Bật relay 2
    \item \texttt{light2\_off}: Tắt relay 2
    \item \texttt{turn\_on}: Bật tất cả relay
    \item \texttt{turn\_off}: Tắt tất cả relay
\end{itemize}

\textbf{Keyboard controls:}
\begin{itemize}
    \item Phím 'q': Thoát ứng dụng
    \item Phím 'r': Reset tất cả relay về trạng thái OFF
    \item ESC: Thoát khẩn cấp
\end{itemize}
